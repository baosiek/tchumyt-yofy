{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 9056410\n"
     ]
    }
   ],
   "source": [
    "# Gets the verdict\n",
    "# url: str = \"https://www.gutenberg.org/ebooks/67237.txt.utf-8\"\n",
    "# urllib.request.urlretrieve(url, \"books/the-verdict.txt\")\n",
    "\n",
    "# Initializes the string that will contain the loaded text above\n",
    "raw_text: str = None\n",
    "\n",
    "# Reads the loaded text\n",
    "with open(\"books/all.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Logs the metadata of the text\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 2444224\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Initializes the tokenizer\n",
    "tokenizer: tiktoken.core.Encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Runs it on the raw text\n",
    "tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size set to: 50257\n"
     ]
    }
   ],
   "source": [
    "# The below parameters define the shape of the embedding layer\n",
    "vocabulary_size: int = tokenizer.n_vocab # => is the input size of the embedding layer, ie, the size of the vocabulary\n",
    "token_embedding_dim: int = 512 # => is the output size of the embedding layer\n",
    "\n",
    "# The below parameters define the shape of the positional embedding layer\n",
    "# The positional embedding layer is of shape context_size x output_dim\n",
    "context_length: int = 8\n",
    "output_dim: int = token_embedding_dim # => is the size of the output of the positional embedding,\n",
    "                                      # that we want to be equal to the token_embedding_dim as this will be the input to the model\n",
    "\n",
    "# The size of the sequence to be generated\n",
    "max_new_tokens: int = 15\n",
    "\n",
    "# The stride\n",
    "stride: int = 1\n",
    "\n",
    "# Batch size\n",
    "batch_size: int = 128\n",
    "\n",
    "# Learning rate\n",
    "lr_rate: float = 0.0005\n",
    "\n",
    "print(f\"Vocabulary size set to: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters of the raw text:\n",
      "\n",
      "\"﻿The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and wit\"\n",
      "\n",
      "Example of text generating with context_size slicing:\n",
      "� ----> �\n",
      "� ----> �\n",
      "﻿ ----> The\n",
      "﻿The ---->  Project\n"
     ]
    }
   ],
   "source": [
    "# Example of using context with the tokenizer\n",
    "print(f\"The first 100 characters of the raw text:\\n\\n\\\"{raw_text[: 180]}\\\"\\n\")\n",
    "print(\"Example of text generating with context_size slicing:\")\n",
    "for i in range(1, 4+1):\n",
    "    context: int = tokens[: i]\n",
    "    desired: int = tokens[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "\n",
    "            # temp_id = token_ids[i + context_length]\n",
    "            # target_tensor = torch.zeros([50257])\n",
    "            # target_tensor[temp_id] = 1.0\n",
    "            # self.target_ids.append(target_tensor)\n",
    "\n",
    "            target_chunk = token_ids[i + 1: i + context_length + 1]            \n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=batch_size, context_length=context_length,\n",
    "    stride=stride, shuffle=True, drop_last=True,\n",
    "    num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, context_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMText(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int\n",
    "    ):\n",
    "        '''\n",
    "        input_size: int -> size of the embedding dimension in the transformer parlance\n",
    "        hidden_size: int -> size of the hidden state\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "\n",
    "        # Input gate (I):\n",
    "        # I_t = sigma(X_t.W_xi + H_t-1.W_hi + b_i)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xi is the weight matrix of X to I gate\n",
    "        self.W_xi: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hi is the weight matrix of h to the I gate\n",
    "        self.W_hi: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_i is the bias to the I gate\n",
    "        self.b_i: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Forget gate (F):\n",
    "        # F_t = sigma(X_t.W_xf + H_t-1.W_hf + b_f)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xf is the weight matrix of X to F gate\n",
    "        self.W_xf: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hf is the weight matrix of h to the F gate\n",
    "        self.W_hf: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_f is the bias to the F gate\n",
    "        self.b_f: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Output gate (O):\n",
    "        # O_t = sigma(X_t.W_xo + H_t-1.W_ho + b_o)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xo is the weight matrix of X to O gate\n",
    "        self.W_xo: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the O gate\n",
    "        self.W_ho: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the O gate\n",
    "        self.b_o: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        \n",
    "        # Cell (C):\n",
    "        # C_t = sigma(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xc is the weight matrix of X to C cell\n",
    "        self.W_xc: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the C cell\n",
    "        self.W_hc: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the C cell\n",
    "        self.b_c: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        # Initializes all weights \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f\"LSTMText(input_size={self.input_size}, hidden_size={self.hidden_size})\"\n",
    "        return repr\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        stdev: float = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, states: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        assumes x.shape represents (batch_size, sequence_size, embedding_dimension)\n",
    "        \"\"\"\n",
    "        bs, sequence_size, input_size = X.size()\n",
    "        \n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Input shape: {input_size} is not equal to model input size: {self.input_size}\")\n",
    "\n",
    "        if states is None:\n",
    "            H_t, C_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device)\n",
    "            )\n",
    "        else:\n",
    "            H_t, C_t = states\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(sequence_size):\n",
    "            x = X[:, t, :]\n",
    "            # I is the input gate\n",
    "            I_t = torch.sigmoid(torch.matmul(x, self.W_xi) + torch.matmul(H_t, self.W_hi) + self.b_i)\n",
    "\n",
    "            # print(f\"X shape: {X.shape}\")\n",
    "            # print(f\"x shape: {x.shape}\")\n",
    "            # print(f\"self.W_xi shape: {self.W_xi.shape}\")\n",
    "            # print(f\"H_t shape: {H_t.shape}\")\n",
    "            # print(f\"self.b_i shape: {self.b_i.shape}\")\n",
    "            # print(f\"I_t shape: {I_t.shape}\")\n",
    "\n",
    "            # F is the forget state\n",
    "            F_t = torch.sigmoid(torch.matmul(x, self.W_xf) + torch.matmul(H_t, self.W_hf) + self.b_f)\n",
    "\n",
    "            # O is the output state\n",
    "            O_t = torch.sigmoid(torch.matmul(x, self.W_xo) + torch.matmul(H_t, self.W_ho) + self.b_o)\n",
    "\n",
    "            # C_t, the memory (C)ell is:\n",
    "            # C_t = F(.)C_t-1 + I_t(.)C_temp\n",
    "            # C_temp = tanh(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "            C_temp = torch.tanh(torch.matmul(x, self.W_xc) + torch.matmul(H_t, self.W_hc) + self.b_c)\n",
    "            C_t = F_t * C_t + I_t * C_temp\n",
    "            H_t = O_t * torch.tanh(C_t)\n",
    "            outputs.append(H_t.unsqueeze(1))\n",
    "\n",
    "        result = torch.cat(outputs, dim=1)\n",
    "        return result, (H_t, C_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 vocabulary_size: int,\n",
    "                 context_length: int,\n",
    "                 output_dim: int,\n",
    "                 out_features: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # The parameters\n",
    "        self.vocabulary_size: int = vocabulary_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.input_size: int = input_size\n",
    "        self.context_length: int = context_length\n",
    "        self.output_dim: int = output_dim\n",
    "        self.output_features: int = out_features\n",
    "\n",
    "        # the embedding layer\n",
    "        self.token_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=output_dim)\n",
    "\n",
    "        # the positional embedding layer\n",
    "        self.pos_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.hidden_size)\n",
    "\n",
    "        # the rnn (LSTM or GRU) rnn\n",
    "        self.rnn_layer: LSTMText = LSTMText(input_size=self.input_size, hidden_size=hidden_size)\n",
    "\n",
    "        # Linear layer to generate the output\n",
    "        self.output_layer: nn.Linear = nn.Linear(in_features=input_size, out_features=out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f'''RNNModel(input_size={self.input_size},\n",
    "        hidden_size={self.hidden_size},\n",
    "        vocabulary_size={self.vocabulary_size},\n",
    "        context_length={self.context_length},\n",
    "        output_dim={self.output_dim},\n",
    "        out_features={self.output_features},\n",
    "        (token_embedding_layer)={self.token_embedding_layer}),\n",
    "        (pos_embedding_layer)={self.pos_embedding_layer},\n",
    "        (rnn_layer)={self.rnn_layer},\n",
    "        (output_layer)={self.output_layer}\n",
    "        )\n",
    "        '''\n",
    "        return repr\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        token_embeddings_ = self.token_embedding_layer(X)\n",
    "        pos_embeddings_ = self.pos_embedding_layer(torch.arange(self.context_length).to(device=device))\n",
    "        input_embeddings_ = token_embeddings_ + pos_embeddings_\n",
    "\n",
    "        output_, _ = self.rnn_layer(input_embeddings_)\n",
    "        # print(f\"Output from LSTM shape: {output_.shape}\")\n",
    "        output_ = self.output_layer(output_)\n",
    "        # print(f\"Output RNN shape: {output_.shape}\")\n",
    "\n",
    "        # output_ = torch.softmax(output_, dim=-1)\n",
    "        # output_ = torch.argmax(output_, dim=-1)\n",
    "        \n",
    "        return output_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 53616721\n",
      "Total memory footprint required to run the model: 204.53 MB\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_size=token_embedding_dim,\n",
    "                 hidden_size=output_dim,\n",
    "                 vocabulary_size=vocabulary_size,\n",
    "                 context_length=context_length,\n",
    "                 output_dim=output_dim,\n",
    "                 out_features=vocabulary_size)\n",
    "model.to(device=device)\n",
    "model.eval()\n",
    "\n",
    "total_params: int = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "total_size_mb: int = (total_params * 4) / (1024 * 1024)\n",
    "print(\"Total memory footprint required to\"\n",
    "          f\" run the model: {total_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim_sequence(input: torch.Tensor, context_length: int):\n",
    "    input = input[:, :context_length]\n",
    "    output = F.pad(input, (0, context_length - input.shape[1]), \"constant\", 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,\n",
    "                         idx: torch.Tensor,\n",
    "                         max_new_tokens,\n",
    "                         context_size):\n",
    "    \n",
    "    if idx.shape[-1] < context_size:\n",
    "        idx = pad_or_trim_sequence(idx, context_length=context_size)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            output_ = model(idx_cond)\n",
    "        \n",
    "        output_ = torch.softmax(output_, dim=-1)\n",
    "        output_ = torch.argmax(output_, dim=-1)\n",
    "        # print(f\"Output from model shape before cat: {output_.shape}\")\n",
    "        # print(f\"output_[:, -1]: {output_[:, 2].unsqueeze(1).shape}\")\n",
    "        # print(f\"Input generate_text_simple idx: {idx.shape}\")\n",
    "        idx = torch.cat((idx, output_[:, -1].unsqueeze(1)), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(texts, tokenizer):\n",
    "    encoded_tensors = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})    \n",
    "        encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "        encoded_tensors.append(encoded_tensor)\n",
    "    return_tensor = torch.cat(encoded_tensors, dim=0).to(device=device)\n",
    "    return return_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    response = []\n",
    "    for batch in range(token_ids.shape[0]):\n",
    "        response.append(\n",
    "            tokenizer.decode(\n",
    "                token_ids[batch, :].squeeze(0).tolist()\n",
    "                )\n",
    "            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = [\"He was always thinking\"] #He was always thinking what the estate would be if those mortgages could but be paid off.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# every effort moves you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " ['He was always thinking!!!! historicgerald declineposedTIT Kung Dick Ahmeduno DavidsononeyNext Tight Madison Hugh']\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.2 Generator to enabling split dataset into train and validation subsets\n",
    "generator1: torch.Generator = torch.Generator().manual_seed(918)\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_length, stride)\n",
    "\n",
    "# 1.3 Creates a list with both subsets, 90% training, 10% evaluation\n",
    "datasets: List[Subset] = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [0.9, 0.1],\n",
    "    generator=generator1\n",
    ")\n",
    "\n",
    "# 1.4 Assigns train and validation datasets accordingly\n",
    "train_dataset: Subset = datasets[0]\n",
    "validation_dataset: Subset = datasets[1]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "evaluator = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.829407 Batches: 0\n",
      "Loss: 10.832548 Batches: 10000\n",
      "End of epoch: 0 Loss: 10.833146 Batches: 17185\n",
      "Output text:\n",
      " ['He was always thinking!!!! historicgerald declineposedTIT Kung Dick Ahmeduno DavidsononeyNext Tight Madison Hugh']\n",
      "Loss: 10.833944 Batches: 20000\n",
      "Loss: 10.837105 Batches: 30000\n",
      "End of epoch: 1 Loss: 10.827229 Batches: 34370\n",
      "Output text:\n",
      " ['He was always thinking!!!! historicgerald declineposedTIT Kung Dick Ahmeduno DavidsononeyNext Tight Madison Hugh']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m target_batch \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m output_flatten: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(output, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m target_batch_flatten: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(target_batch, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     49\u001b[0m pos_embeddings_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding_layer(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     50\u001b[0m input_embeddings_ \u001b[38;5;241m=\u001b[39m token_embeddings_ \u001b[38;5;241m+\u001b[39m pos_embeddings_\n\u001b[0;32m---> 52\u001b[0m output_, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embeddings_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(f\"Output from LSTM shape: {output_.shape}\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m output_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(output_)\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 118\u001b[0m, in \u001b[0;36mLSTMText.forward\u001b[0;34m(self, X, states)\u001b[0m\n\u001b[1;32m    108\u001b[0m I_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_xi) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(H_t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hi) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_i)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# print(f\"X shape: {X.shape}\")\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(f\"x shape: {x.shape}\")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(f\"self.W_xi shape: {self.W_xi.shape}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# F is the forget state\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m F_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_xf) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(H_t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_hf) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_f\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# O is the output state\u001b[39;00m\n\u001b[1;32m    121\u001b[0m O_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_xo) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(H_t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_ho) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_o)\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_batch = input_batch.to(device=device)\n",
    "        target_batch = target_batch.to(device=device)\n",
    "        output = model(input_batch)\n",
    "\n",
    "        output_flatten: torch.Tensor = torch.flatten(output, 0, 1)\n",
    "        target_batch_flatten: torch.Tensor = torch.flatten(target_batch, 0, 1)\n",
    "        # print(f\"Model output flatten shape: {output_flatten.shape}\")\n",
    "        # print(f\"target_batch flatten shape: {target_batch_flatten.shape}\")\n",
    "        loss: torch.Tensor = evaluator(output_flatten, target_batch_flatten)\n",
    "        loss.backward()\n",
    "\n",
    "        model.eval()\n",
    "        # perf = performance(input_batch, target_batch)\n",
    "        model.train()\n",
    "\n",
    "        if counter % 10000 == 0:\n",
    "            print(f\"Loss: {loss:.6f} Batches: {counter}\")\n",
    "        counter += 1\n",
    "\n",
    "    print(f\"End of epoch: {epoch} Loss: {loss:.6f} Batches: {counter}\")\n",
    "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "    \n",
    "    token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
