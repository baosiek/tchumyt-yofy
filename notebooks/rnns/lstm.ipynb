{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size: int = 1\n",
    "hidden_size: int = 4\n",
    "batch_sz: int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "X, W_xh = torch.randn(batch_sz, seq_size), torch.randn(seq_size, hidden_size)\n",
    "H, W_hh = torch.randn(batch_sz, hidden_size), torch.randn(hidden_size, hidden_size)\n",
    "mul = torch.matmul(X, W_xh) + torch.matmul(H, W_hh)\n",
    "\n",
    "print(H.shape, mul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int\n",
    "    ):\n",
    "        '''\n",
    "        input_size: int -> size of the vocabulary\n",
    "        hidden_size: int -> size of the hidden state, ie, embedding dimension\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "\n",
    "        # Input gate (I):\n",
    "        # I_t = sigma(X_t.W_xi + H_t-1.W_hi + b_i)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xi is the weight matrix of X to I gate\n",
    "        self.W_xi: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hi is the weight matrix of h to the I gate\n",
    "        self.W_hi: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_i is the bias to the I gate\n",
    "        self.b_i: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Forget gate (F):\n",
    "        # F_t = sigma(X_t.W_xf + H_t-1.W_hf + b_f)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xf is the weight matrix of X to F gate\n",
    "        self.W_xf: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hf is the weight matrix of h to the F gate\n",
    "        self.W_hf: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_f is the bias to the F gate\n",
    "        self.b_f: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Output gate (O):\n",
    "        # O_t = sigma(X_t.W_xo + H_t-1.W_ho + b_o)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xo is the weight matrix of X to O gate\n",
    "        self.W_xo: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the O gate\n",
    "        self.W_ho: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the O gate\n",
    "        self.b_o: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        \n",
    "        # Cell (C):\n",
    "        # C_t = sigma(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xc is the weight matrix of X to C cell\n",
    "        self.W_xc: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the C cell\n",
    "        self.W_hc: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the C cell\n",
    "        self.b_c: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        # Initializes all weights \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        stdev: float = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, states: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        assumes x.shape represents (batch_size, sequence_size, embedding_dimension)\n",
    "        \"\"\"\n",
    "        bs, sequence_size, input_size = X.size()\n",
    "\n",
    "        if states is None:\n",
    "            H_t, C_t = (\n",
    "                torch.zeros(bs, hidden_size).to(device=X.device),\n",
    "                torch.zeros(bs, hidden_size).to(device=X.device)\n",
    "            )\n",
    "        else:\n",
    "            H_t, C_t = states\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(sequence_size):\n",
    "            x = X[:, t, :]\n",
    "            # I is the input gate\n",
    "            I_t = torch.sigmoid(torch.matmul(x, self.W_xi) + torch.matmul(H_t, self.W_hi) + self.b_i)\n",
    "\n",
    "            # F is the forget state\n",
    "            F_t = torch.sigmoid(torch.matmul(x, self.W_xf) + torch.matmul(H_t, self.W_hf) + self.b_f)\n",
    "\n",
    "            # O is the output state\n",
    "            O_t = torch.sigmoid(torch.matmul(x, self.W_xo) + torch.matmul(H_t, self.W_ho) + self.b_o)\n",
    "\n",
    "            # C_t, the memory (C)ell is:\n",
    "            # C_t = F(.)C_t-1 + I_t(.)C_temp\n",
    "            # C_temp = tanh(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "            C_temp = torch.tanh(torch.matmul(x, self.W_xc) + torch.matmul(H_t, self.W_hc) + self.b_c)\n",
    "            C_t = F_t * C_t + I_t * C_temp\n",
    "            H_t = O_t * torch.tanh(C_t)\n",
    "            outputs.append(H_t)\n",
    "        return outputs, (H_t, C_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xi.shape: torch.Size([50726, 768])\n"
     ]
    }
   ],
   "source": [
    "seq_size: int = 3 # the size of the sequence\n",
    "hidden_size: int = 768 # the embedding dimension\n",
    "batch_sz: int = 10\n",
    "input_size: int = 50726 # size of the vocabulary\n",
    "lstm = NaiveLSTM(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "input: torch.Tensor = torch.ones((batch_sz, seq_size, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, states = lstm(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
