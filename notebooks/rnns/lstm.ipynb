{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 357726\n"
     ]
    }
   ],
   "source": [
    "# Gets the verdict\n",
    "url: str = \"https://www.gutenberg.org/ebooks/67237.txt.utf-8\"\n",
    "urllib.request.urlretrieve(url, \"books/the-verdict.txt\")\n",
    "\n",
    "# Initializes the string that will contain the loaded text above\n",
    "raw_text: str = None\n",
    "\n",
    "# Reads the loaded text\n",
    "with open(\"books/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Logs the metadata of the text\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 97421\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Initializes the tokenizer\n",
    "tokenizer: tiktoken.core.Encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Runs it on the raw text\n",
    "tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size set to: 50257\n"
     ]
    }
   ],
   "source": [
    "# The below parameters define the shape of the embedding layer\n",
    "vocabulary_size: int = tokenizer.n_vocab # => is the input size of the embedding layer, ie, the size of the vocabulary\n",
    "token_embedding_dim: int = 256 # => is the output size of the embedding layer\n",
    "\n",
    "# The below parameters define the shape of the positional embedding layer\n",
    "# The positional embedding layer is of shape context_size x output_dim\n",
    "context_length: int = 10\n",
    "output_dim: int = token_embedding_dim # => is the size of the output of the positional embedding,\n",
    "                                      # that we want to be equal to the token_embedding_dim as this will be the input to the model\n",
    "\n",
    "# The size of the sequence to be generated\n",
    "max_new_tokens: int = 15\n",
    "\n",
    "# The stride\n",
    "stride: int = 1\n",
    "\n",
    "# Batch size\n",
    "batch_size: int = 64\n",
    "\n",
    "print(f\"Vocabulary size set to: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters of the raw text:\n",
      "\n",
      "\"﻿The Project Gutenberg eBook of An open verdict\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost \"\n",
      "\n",
      "Example of text generating with context_size slicing:\n",
      "� ----> �\n",
      "� ----> �\n",
      "﻿ ----> The\n",
      "﻿The ---->  Project\n"
     ]
    }
   ],
   "source": [
    "# Example of using context with the tokenizer\n",
    "print(f\"The first 100 characters of the raw text:\\n\\n\\\"{raw_text[: 180]}\\\"\\n\")\n",
    "print(\"Example of text generating with context_size slicing:\")\n",
    "for i in range(1, 4+1):\n",
    "    context: int = tokens[: i]\n",
    "    desired: int = tokens[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "\n",
    "            temp_id = token_ids[i + context_length]\n",
    "            target_tensor = torch.zeros([50257])\n",
    "            target_tensor[temp_id] = 1.0\n",
    "            self.target_ids.append(target_tensor)\n",
    "\n",
    "            # target_chunk = token_ids[i + 1: i + max_length + 1]            \n",
    "            # self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=batch_size, context_length=context_length,\n",
    "    stride=stride, shuffle=True, drop_last=True,\n",
    "    num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, context_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMText(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int\n",
    "    ):\n",
    "        '''\n",
    "        input_size: int -> size of the embedding dimension in the transformer parlance\n",
    "        hidden_size: int -> size of the hidden state\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "\n",
    "        # Input gate (I):\n",
    "        # I_t = sigma(X_t.W_xi + H_t-1.W_hi + b_i)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xi is the weight matrix of X to I gate\n",
    "        self.W_xi: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hi is the weight matrix of h to the I gate\n",
    "        self.W_hi: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_i is the bias to the I gate\n",
    "        self.b_i: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Forget gate (F):\n",
    "        # F_t = sigma(X_t.W_xf + H_t-1.W_hf + b_f)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xf is the weight matrix of X to F gate\n",
    "        self.W_xf: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hf is the weight matrix of h to the F gate\n",
    "        self.W_hf: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_f is the bias to the F gate\n",
    "        self.b_f: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Output gate (O):\n",
    "        # O_t = sigma(X_t.W_xo + H_t-1.W_ho + b_o)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xo is the weight matrix of X to O gate\n",
    "        self.W_xo: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the O gate\n",
    "        self.W_ho: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the O gate\n",
    "        self.b_o: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        \n",
    "        # Cell (C):\n",
    "        # C_t = sigma(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xc is the weight matrix of X to C cell\n",
    "        self.W_xc: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the C cell\n",
    "        self.W_hc: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the C cell\n",
    "        self.b_c: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        # Initializes all weights \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f\"LSTMText(input_size={self.input_size}, hidden_size={self.hidden_size})\"\n",
    "        return repr\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        stdev: float = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, states: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        assumes x.shape represents (batch_size, sequence_size, embedding_dimension)\n",
    "        \"\"\"\n",
    "        bs, sequence_size, input_size = X.size()\n",
    "        \n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Input shape: {input_size} is not equal to model input size: {self.input_size}\")\n",
    "\n",
    "        if states is None:\n",
    "            H_t, C_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device)\n",
    "            )\n",
    "        else:\n",
    "            H_t, C_t = states\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(sequence_size):\n",
    "            x = X[:, t, :]\n",
    "            # I is the input gate\n",
    "            I_t = torch.sigmoid(torch.matmul(x, self.W_xi) + torch.matmul(H_t, self.W_hi) + self.b_i)\n",
    "\n",
    "            # F is the forget state\n",
    "            F_t = torch.sigmoid(torch.matmul(x, self.W_xf) + torch.matmul(H_t, self.W_hf) + self.b_f)\n",
    "\n",
    "            # O is the output state\n",
    "            O_t = torch.sigmoid(torch.matmul(x, self.W_xo) + torch.matmul(H_t, self.W_ho) + self.b_o)\n",
    "\n",
    "            # C_t, the memory (C)ell is:\n",
    "            # C_t = F(.)C_t-1 + I_t(.)C_temp\n",
    "            # C_temp = tanh(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "            C_temp = torch.tanh(torch.matmul(x, self.W_xc) + torch.matmul(H_t, self.W_hc) + self.b_c)\n",
    "            C_t = F_t * C_t + I_t * C_temp\n",
    "            H_t = O_t * torch.tanh(C_t)\n",
    "            outputs.append(H_t)\n",
    "\n",
    "        result = torch.cat([outputs[-1]], dim=0)\n",
    "        return result, (H_t, C_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 vocabulary_size: int,\n",
    "                 context_length: int,\n",
    "                 output_dim: int,\n",
    "                 out_features: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # The parameters\n",
    "        self.vocabulary_size: int = vocabulary_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.input_size: int = input_size\n",
    "        self.context_length: int = context_length\n",
    "        self.output_dim: int = output_dim\n",
    "        self.output_features: int = out_features\n",
    "\n",
    "        # the embedding layer\n",
    "        self.token_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=output_dim)\n",
    "\n",
    "        # the positional embedding layer\n",
    "        self.pos_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.hidden_size)\n",
    "\n",
    "        # the rnn (LSTM or GRU) rnn\n",
    "        self.rnn_layer: LSTMText = LSTMText(input_size=self.input_size, hidden_size=hidden_size)\n",
    "\n",
    "        # Linear layer to generate the output\n",
    "        self.output_layer: nn.Linear = nn.Linear(in_features=input_size, out_features=out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f'''RNNModel(input_size={self.input_size},\n",
    "        hidden_size={self.hidden_size},\n",
    "        vocabulary_size={self.vocabulary_size},\n",
    "        context_length={self.context_length},\n",
    "        output_dim={self.output_dim},\n",
    "        out_features={self.output_features},\n",
    "        (token_embedding_layer)={self.token_embedding_layer}),\n",
    "        (pos_embedding_layer)={self.pos_embedding_layer},\n",
    "        (rnn_layer)={self.rnn_layer},\n",
    "        (output_layer)={self.output_layer}\n",
    "        )\n",
    "        '''\n",
    "        return repr\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        token_embeddings_ = self.token_embedding_layer(X)\n",
    "        pos_embeddings_ = self.pos_embedding_layer(torch.arange(self.context_length))\n",
    "        input_embeddings_ = token_embeddings_ + pos_embeddings_\n",
    "\n",
    "        output_, _ = self.rnn_layer(input_embeddings_)\n",
    "        output_ = self.output_layer(output_)\n",
    "        \n",
    "        return output_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device: str = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 26309713\n",
      "Total memory footprint required to run the model: 100.36 MB\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_size=token_embedding_dim,\n",
    "                 hidden_size=output_dim,\n",
    "                 vocabulary_size=vocabulary_size,\n",
    "                 context_length=context_length,\n",
    "                 output_dim=output_dim,\n",
    "                 out_features=vocabulary_size)\n",
    "model.eval()\n",
    "\n",
    "total_params: int = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "total_size_mb: int = (total_params * 4) / (1024 * 1024)\n",
    "print(\"Total memory footprint required to\"\n",
    "          f\" run the model: {total_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim_sequence(input: torch.Tensor, context_length: int):\n",
    "    input = input[:, :context_length]\n",
    "    output = F.pad(input, (0, context_length - input.shape[1]), \"constant\", 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,\n",
    "                         idx: torch.Tensor,\n",
    "                         max_new_tokens,\n",
    "                         context_size):\n",
    "    \n",
    "    if idx.shape[-1] < context_size:\n",
    "        idx = pad_or_trim_sequence(idx, context_length=context_size)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            output_ = model(idx_cond)\n",
    "            print(f\"Output shape: {output_.shape}\")\n",
    "        \n",
    "        output_ = torch.softmax(output_, dim=-1)\n",
    "        output_ = torch.argmax(output_, dim=-1)\n",
    "        idx = torch.cat((idx, output_.unsqueeze(0)), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"He was always thinking\" # what the estate would be if those mortgages could but be paid off.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output shape: torch.Size([1, 50257])\n",
      "Output text:\n",
      " He was always thinking!!!!!! surgeon60female Ribbon unusually &&ades Gold TeeörTrain Bullet Casey Ryan Ryan\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.2 Generator to enabling split dataset into train and validation subsets\n",
    "generator1: torch.Generator = torch.Generator().manual_seed(918)\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_length, stride)\n",
    "\n",
    "# 1.3 Creates a list with both subsets, 90% training, 10% evaluation\n",
    "datasets: List[Subset] = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [0.9, 0.1],\n",
    "    generator=generator1\n",
    ")\n",
    "\n",
    "# 1.4 Assigns train and validation datasets accordingly\n",
    "train_dataset: Subset = datasets[0]\n",
    "validation_dataset: Subset = datasets[1]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "evaluator = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X: torch.Tensor, y: torch.Tensor):\n",
    "    output = model(X)\n",
    "    perf = torch.nn.functional.cross_entropy(\n",
    "            output,\n",
    "            y\n",
    "        )\n",
    "    \n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 10.83252239227295\n",
      "Performance: 10.843337059020996\n",
      "End of epoch: 0\n",
      "Output text:\n",
      " He was always thinking!!!!!! surgeon60female Ribbon unusually &&ades Gold TeeörTrain Bullet Casey Ryan Ryan\n",
      "Performance: 10.830931663513184\n",
      "End of epoch: 1\n",
      "Output text:\n",
      " He was always thinking!!!!!! surgeon60female Ribbon unusually &&ades Gold TeeörTrain Bullet Casey Ryan Ryan\n",
      "Performance: 10.853130340576172\n",
      "Performance: 10.855406761169434\n",
      "End of epoch: 2\n",
      "Output text:\n",
      " He was always thinking!!!!!! surgeon60female Ribbon unusually &&ades Gold TeeörTrain Bullet Casey Ryan Ryan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_batch)\n\u001b[1;32m      9\u001b[0m loss: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m evaluator(output, target_batch)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m perf \u001b[38;5;241m=\u001b[39m performance(input_batch, target_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tmyt/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_batch)\n",
    "        loss: torch.Tensor = evaluator(output, target_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        model.eval()\n",
    "        perf = performance(input_batch, target_batch)\n",
    "        model.train()\n",
    "\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Performance: {perf}\")\n",
    "        counter += 1\n",
    "\n",
    "    print(f\"End of epoch: {epoch}\")\n",
    "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "    \n",
    "    token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
