{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 357726\n"
     ]
    }
   ],
   "source": [
    "# Gets the verdict\n",
    "url: str = \"https://www.gutenberg.org/ebooks/67237.txt.utf-8\"\n",
    "urllib.request.urlretrieve(url, \"books/the-verdict.txt\")\n",
    "\n",
    "# Initializes the string that will contain the loaded text above\n",
    "raw_text: str = None\n",
    "\n",
    "# Reads the loaded text\n",
    "with open(\"books/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Logs the metadata of the text\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 97421\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Initializes the tokenizer\n",
    "tokenizer: tiktoken.core.Encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Runs it on the raw text\n",
    "tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size set to: 50257\n"
     ]
    }
   ],
   "source": [
    "# The below parameters define the shape of the embedding layer\n",
    "vocabulary_size: int = tokenizer.n_vocab # => is the input size of the embedding layer, ie, the size of the vocabulary\n",
    "token_embedding_dim: int = 256 # => is the output size of the embedding layer\n",
    "\n",
    "# The below parameters define the shape of the positional embedding layer\n",
    "# The positional embedding layer is of shape context_size x output_dim\n",
    "context_length: int = 10\n",
    "output_dim: int = token_embedding_dim # => is the size of the output of the positional embedding,\n",
    "                                      # that we want to be equal to the token_embedding_dim as this will be the input to the model\n",
    "\n",
    "# The size of the sequence to be generated\n",
    "max_new_tokens: int = 15\n",
    "\n",
    "# The stride\n",
    "stride: int = 1\n",
    "\n",
    "# Batch size\n",
    "batch_size: int = 64\n",
    "\n",
    "print(f\"Vocabulary size set to: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters of the raw text:\n",
      "\n",
      "\"﻿The Project Gutenberg eBook of An open verdict\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost \"\n",
      "\n",
      "Example of text generating with context_size slicing:\n",
      "� ----> �\n",
      "� ----> �\n",
      "﻿ ----> The\n",
      "﻿The ---->  Project\n"
     ]
    }
   ],
   "source": [
    "# Example of using context with the tokenizer\n",
    "print(f\"The first 100 characters of the raw text:\\n\\n\\\"{raw_text[: 180]}\\\"\\n\")\n",
    "print(\"Example of text generating with context_size slicing:\")\n",
    "for i in range(1, 4+1):\n",
    "    context: int = tokens[: i]\n",
    "    desired: int = tokens[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "\n",
    "            temp_id = token_ids[i + context_length]\n",
    "            target_tensor = torch.zeros([50257])\n",
    "            target_tensor[temp_id] = 1.0\n",
    "            self.target_ids.append(target_tensor)\n",
    "\n",
    "            # target_chunk = token_ids[i + 1: i + max_length + 1]            \n",
    "            # self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=batch_size, context_length=context_length,\n",
    "    stride=stride, shuffle=True, drop_last=True,\n",
    "    num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, context_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMText(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int\n",
    "    ):\n",
    "        '''\n",
    "        input_size: int -> size of the embedding dimension in the transformer parlance\n",
    "        hidden_size: int -> size of the hidden state\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "\n",
    "        # Input gate (I):\n",
    "        # I_t = sigma(X_t.W_xi + H_t-1.W_hi + b_i)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xi is the weight matrix of X to I gate\n",
    "        self.W_xi: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hi is the weight matrix of h to the I gate\n",
    "        self.W_hi: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_i is the bias to the I gate\n",
    "        self.b_i: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Forget gate (F):\n",
    "        # F_t = sigma(X_t.W_xf + H_t-1.W_hf + b_f)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xf is the weight matrix of X to F gate\n",
    "        self.W_xf: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hf is the weight matrix of h to the F gate\n",
    "        self.W_hf: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_f is the bias to the F gate\n",
    "        self.b_f: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Output gate (O):\n",
    "        # O_t = sigma(X_t.W_xo + H_t-1.W_ho + b_o)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xo is the weight matrix of X to O gate\n",
    "        self.W_xo: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the O gate\n",
    "        self.W_ho: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the O gate\n",
    "        self.b_o: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        \n",
    "        # Cell (C):\n",
    "        # C_t = sigma(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xc is the weight matrix of X to C cell\n",
    "        self.W_xc: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the C cell\n",
    "        self.W_hc: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the C cell\n",
    "        self.b_c: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        # Initializes all weights \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f\"LSTMText(input_size={self.input_size}, hidden_size={self.hidden_size})\"\n",
    "        return repr\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        stdev: float = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, states: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        assumes x.shape represents (batch_size, sequence_size, embedding_dimension)\n",
    "        \"\"\"\n",
    "        bs, sequence_size, input_size = X.size()\n",
    "        \n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Input shape: {input_size} is not equal to model input size: {self.input_size}\")\n",
    "\n",
    "        if states is None:\n",
    "            H_t, C_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device)\n",
    "            )\n",
    "        else:\n",
    "            H_t, C_t = states\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(sequence_size):\n",
    "            x = X[:, t, :]\n",
    "            # I is the input gate\n",
    "            I_t = torch.sigmoid(torch.matmul(x, self.W_xi) + torch.matmul(H_t, self.W_hi) + self.b_i)\n",
    "\n",
    "            # F is the forget state\n",
    "            F_t = torch.sigmoid(torch.matmul(x, self.W_xf) + torch.matmul(H_t, self.W_hf) + self.b_f)\n",
    "\n",
    "            # O is the output state\n",
    "            O_t = torch.sigmoid(torch.matmul(x, self.W_xo) + torch.matmul(H_t, self.W_ho) + self.b_o)\n",
    "\n",
    "            # C_t, the memory (C)ell is:\n",
    "            # C_t = F(.)C_t-1 + I_t(.)C_temp\n",
    "            # C_temp = tanh(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "            C_temp = torch.tanh(torch.matmul(x, self.W_xc) + torch.matmul(H_t, self.W_hc) + self.b_c)\n",
    "            C_t = F_t * C_t + I_t * C_temp\n",
    "            H_t = O_t * torch.tanh(C_t)\n",
    "            outputs.append(H_t)\n",
    "\n",
    "        result = torch.cat([outputs[-1]], dim=0)\n",
    "        return result, (H_t, C_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 vocabulary_size: int,\n",
    "                 context_length: int,\n",
    "                 output_dim: int,\n",
    "                 out_features: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # The parameters\n",
    "        self.vocabulary_size: int = vocabulary_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.input_size: int = input_size\n",
    "        self.context_length: int = context_length\n",
    "        self.output_dim: int = output_dim\n",
    "        self.output_features: int = out_features\n",
    "\n",
    "        # the embedding layer\n",
    "        self.token_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=output_dim)\n",
    "\n",
    "        # the positional embedding layer\n",
    "        self.pos_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.hidden_size)\n",
    "\n",
    "        # the rnn (LSTM or GRU) rnn\n",
    "        self.rnn_layer: LSTMText = LSTMText(input_size=self.input_size, hidden_size=hidden_size)\n",
    "\n",
    "        # Linear layer to generate the output\n",
    "        self.output_layer: nn.Linear = nn.Linear(in_features=input_size, out_features=out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f'''RNNModel(input_size={self.input_size},\n",
    "        hidden_size={self.hidden_size},\n",
    "        vocabulary_size={self.vocabulary_size},\n",
    "        context_length={self.context_length},\n",
    "        output_dim={self.output_dim},\n",
    "        out_features={self.output_features},\n",
    "        (token_embedding_layer)={self.token_embedding_layer}),\n",
    "        (pos_embedding_layer)={self.pos_embedding_layer},\n",
    "        (rnn_layer)={self.rnn_layer},\n",
    "        (output_layer)={self.output_layer}\n",
    "        )\n",
    "        '''\n",
    "        return repr\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        token_embeddings_ = self.token_embedding_layer(X)\n",
    "        pos_embeddings_ = self.pos_embedding_layer(torch.arange(self.context_length))\n",
    "        input_embeddings_ = token_embeddings_ + pos_embeddings_\n",
    "\n",
    "        output_, _ = self.rnn_layer(input_embeddings_)\n",
    "        output_ = self.output_layer(output_)\n",
    "        \n",
    "        return output_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 26309713\n",
      "Total memory footprint required to run the model: 100.36 MB\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_size=token_embedding_dim,\n",
    "                 hidden_size=output_dim,\n",
    "                 vocabulary_size=vocabulary_size,\n",
    "                 context_length=context_length,\n",
    "                 output_dim=output_dim,\n",
    "                 out_features=vocabulary_size)\n",
    "model.eval()\n",
    "\n",
    "total_params: int = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "total_size_mb: int = (total_params * 4) / (1024 * 1024)\n",
    "print(\"Total memory footprint required to\"\n",
    "          f\" run the model: {total_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim_sequence(input: torch.Tensor, context_length: int):\n",
    "    input = input[:, :context_length]\n",
    "    output = F.pad(input, (0, context_length - input.shape[1]), \"constant\", 0)\n",
    "    return output\n",
    "    # print(f\"{output} shape: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,\n",
    "                         idx: torch.Tensor,\n",
    "                         max_new_tokens,\n",
    "                         context_size):\n",
    "    \n",
    "    if idx.shape[-1] < context_size:\n",
    "        idx = pad_or_trim_sequence(idx, context_length=context_size)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            output_ = model(idx_cond)\n",
    "        \n",
    "        output_ = torch.softmax(output_, dim=-1)\n",
    "        output_ = torch.argmax(output_, dim=-1)\n",
    "        idx = torch.cat((idx, output_.unsqueeze(0)), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"He was always thinking\" # what the estate would be if those mortgages could but be paid off.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " He was always think!!!!!! mentallymuch lubric idiotConclusion PP condolences Ningwarn trap Trayvon stim Princ resists DLC\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.2 Generator to enabling split dataset into train and validation subsets\n",
    "generator1: torch.Generator = torch.Generator().manual_seed(918)\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_length, stride)\n",
    "\n",
    "# 1.3 Creates a list with both subsets, 90% training, 10% evaluation\n",
    "datasets: List[Subset] = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [0.9, 0.1],\n",
    "    generator=generator1\n",
    ")\n",
    "\n",
    "# 1.4 Assigns train and validation datasets accordingly\n",
    "train_dataset: Subset = datasets[0]\n",
    "validation_dataset: Subset = datasets[1]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "evaluator = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X: torch.Tensor, y: torch.Tensor):\n",
    "    output = model(X)\n",
    "    perf = torch.nn.functional.cross_entropy(\n",
    "            output,\n",
    "            y\n",
    "        )\n",
    "    \n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_batch)\n",
    "        loss: torch.Tensor = evaluator(output, target_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        perf = performance(input_batch, target_batch)\n",
    "\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Performance: {perf}\")\n",
    "        counter += 1\n",
    "\n",
    "    print(f\"End of epoch: {epoch}\")\n",
    "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "    \n",
    "    token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
