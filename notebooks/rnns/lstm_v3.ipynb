{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 18090147\n"
     ]
    }
   ],
   "source": [
    "# Gets the verdict\n",
    "# url: str = \"https://www.gutenberg.org/ebooks/67237.txt.utf-8\"\n",
    "# urllib.request.urlretrieve(url, \"books/the-verdict.txt\")\n",
    "\n",
    "# Initializes the string that will contain the loaded text above\n",
    "raw_text: str = None\n",
    "\n",
    "# Reads the loaded text\n",
    "with open(\"books/all.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Logs the metadata of the text\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4945337\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Initializes the tokenizer\n",
    "tokenizer: tiktoken.core.Encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Runs it on the raw text\n",
    "tokens = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size set to: 50257\n"
     ]
    }
   ],
   "source": [
    "# The below parameters define the shape of the embedding layer\n",
    "vocabulary_size: int = tokenizer.n_vocab # => is the input size of the embedding layer, ie, the size of the vocabulary\n",
    "token_embedding_dim: int = 1024 # => is the output size of the embedding layer\n",
    "\n",
    "# The below parameters define the shape of the positional embedding layer\n",
    "# The positional embedding layer is of shape context_size x output_dim\n",
    "context_length: int = 16\n",
    "output_dim: int = token_embedding_dim # => is the size of the output of the positional embedding,\n",
    "                                      # that we want to be equal to the token_embedding_dim as this will be the input to the model\n",
    "\n",
    "# The size of the sequence to be generated\n",
    "max_new_tokens: int = 15\n",
    "\n",
    "# The stride\n",
    "stride: int = 1\n",
    "\n",
    "# Batch size\n",
    "batch_size: int = 256\n",
    "\n",
    "# Learning rate\n",
    "lr_rate: float = 0.000005\n",
    "\n",
    "# Number of LSTM layers\n",
    "num_layers: int = 6\n",
    "\n",
    "# Number of epochs to train the model\n",
    "num_epochs: int = 10\n",
    "\n",
    "print(f\"Vocabulary size set to: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters of the raw text:\n",
      "\n",
      "\"﻿The Project Gutenberg eBook of The Adventures of Sherlock Holmes\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cos\"\n",
      "\n",
      "Example of text generating with context_size slicing:\n",
      "� ----> �\n",
      "� ----> �\n",
      "﻿ ----> The\n",
      "﻿The ---->  Project\n"
     ]
    }
   ],
   "source": [
    "# Example of using context with the tokenizer\n",
    "print(f\"The first 100 characters of the raw text:\\n\\n\\\"{raw_text[: 180]}\\\"\\n\")\n",
    "print(\"Example of text generating with context_size slicing:\")\n",
    "for i in range(1, 4+1):\n",
    "    context: int = tokens[: i]\n",
    "    desired: int = tokens[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "\n",
    "            # temp_id = token_ids[i + context_length]\n",
    "            # target_tensor = torch.zeros([50257])\n",
    "            # target_tensor[temp_id] = 1.0\n",
    "            # self.target_ids.append(target_tensor)\n",
    "\n",
    "            target_chunk = token_ids[i + 1: i + context_length + 1]            \n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=batch_size, context_length=context_length,\n",
    "    stride=stride, shuffle=True, drop_last=True,\n",
    "    num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, context_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMText(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            id: int,\n",
    "            input_size: int,\n",
    "            hidden_size: int\n",
    "    ):\n",
    "        '''\n",
    "        input_size: int -> size of the embedding dimension in the transformer parlance\n",
    "        hidden_size: int -> size of the hidden state\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.id: int = id\n",
    "        self.input_size: int = input_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "\n",
    "        # Input gate (I):\n",
    "        # I_t = sigma(X_t.W_xi + H_t-1.W_hi + b_i)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xi is the weight matrix of X to I gate\n",
    "        self.W_xi: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hi is the weight matrix of h to the I gate\n",
    "        self.W_hi: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_i is the bias to the I gate\n",
    "        self.b_i: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Forget gate (F):\n",
    "        # F_t = sigma(X_t.W_xf + H_t-1.W_hf + b_f)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xf is the weight matrix of X to F gate\n",
    "        self.W_xf: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_hf is the weight matrix of h to the F gate\n",
    "        self.W_hf: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_f is the bias to the F gate\n",
    "        self.b_f: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "\n",
    "        # Output gate (O):\n",
    "        # O_t = sigma(X_t.W_xo + H_t-1.W_ho + b_o)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xo is the weight matrix of X to O gate\n",
    "        self.W_xo: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the O gate\n",
    "        self.W_ho: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the O gate\n",
    "        self.b_o: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        \n",
    "        # Cell (C):\n",
    "        # C_t = sigma(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "        # Where:\n",
    "        # X_t is X(the sequence) at time t\n",
    "        # H_t-1 is H(the hidden state) at time t-1\n",
    "        # W_xc is the weight matrix of X to C cell\n",
    "        self.W_xc: nn.Parameter = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size))\n",
    "\n",
    "        # W_ho is the weight matrix of h to the C cell\n",
    "        self.W_hc: torch.Tensor = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # self.b_o is the bias to the C cell\n",
    "        self.b_c: torch.Tensor = nn.Parameter(torch.Tensor( self.hidden_size))\n",
    "\n",
    "        # Initializes all weights \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f\"LSTMText(id={self.id}, input_size={self.input_size}, hidden_size={self.hidden_size})\"\n",
    "        return repr\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        stdev: float = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, states: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        assumes x.shape represents (batch_size, sequence_size, embedding_dimension)\n",
    "        \"\"\"\n",
    "        bs, sequence_size, input_size = X.size()\n",
    "        \n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Input shape: {input_size} is not equal to model input size: {self.input_size}\")\n",
    "\n",
    "        if states is None:\n",
    "            H_t, C_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(device=X.device)\n",
    "            )\n",
    "        else:\n",
    "            H_t, C_t = states\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(sequence_size):\n",
    "            x = X[:, t, :]\n",
    "            # I is the input gate\n",
    "            I_t = torch.sigmoid(torch.matmul(x, self.W_xi) + torch.matmul(H_t, self.W_hi) + self.b_i)\n",
    "\n",
    "            # print(f\"X shape: {X.shape}\")\n",
    "            # print(f\"x shape: {x.shape}\")\n",
    "            # print(f\"self.W_xi shape: {self.W_xi.shape}\")\n",
    "            # print(f\"H_t shape: {H_t.shape}\")\n",
    "            # print(f\"self.b_i shape: {self.b_i.shape}\")\n",
    "            # print(f\"I_t shape: {I_t.shape}\")\n",
    "\n",
    "            # F is the forget state\n",
    "            F_t = torch.sigmoid(torch.matmul(x, self.W_xf) + torch.matmul(H_t, self.W_hf) + self.b_f)\n",
    "\n",
    "            # O is the output state\n",
    "            O_t = torch.sigmoid(torch.matmul(x, self.W_xo) + torch.matmul(H_t, self.W_ho) + self.b_o)\n",
    "\n",
    "            # C_t, the memory (C)ell is:\n",
    "            # C_t = F(.)C_t-1 + I_t(.)C_temp\n",
    "            # C_temp = tanh(X_t.W_xc + H_t-1.W_hc + b_c)\n",
    "            C_temp = torch.tanh(torch.matmul(x, self.W_xc) + torch.matmul(H_t, self.W_hc) + self.b_c)\n",
    "            C_t = F_t * C_t + I_t * C_temp\n",
    "            H_t = O_t * torch.tanh(C_t)\n",
    "            outputs.append(H_t.unsqueeze(1))\n",
    "\n",
    "        result = torch.cat(outputs, dim=1)\n",
    "        return result, (H_t, C_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 vocabulary_size: int,\n",
    "                 context_length: int,\n",
    "                 output_dim: int,\n",
    "                 out_features: int,\n",
    "                 num_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # The parameters\n",
    "        self.vocabulary_size: int = vocabulary_size\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.input_size: int = input_size\n",
    "        self.context_length: int = context_length\n",
    "        self.output_dim: int = output_dim\n",
    "        self.output_features: int = out_features\n",
    "        self.num_layers: int = num_layers\n",
    "\n",
    "        # the embedding layer\n",
    "        self.token_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=output_dim)\n",
    "\n",
    "        # the positional embedding layer\n",
    "        self.pos_embedding_layer: nn.Embedding = nn.Embedding(num_embeddings=self.context_length, embedding_dim=self.hidden_size)\n",
    "\n",
    "        # the rnn (LSTM or GRU) rnn\n",
    "        self.rnn_layers: nn.ModuleList = nn.ModuleList(\n",
    "            [LSTMText(id=id, input_size=self.input_size, hidden_size=hidden_size) for id in range(self.num_layers)])\n",
    "\n",
    "        # Linear layer to generate the output\n",
    "        self.output_layer: nn.Linear = nn.Linear(in_features=input_size, out_features=out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr: str = f'''RNNModel(input_size={self.input_size},\n",
    "        hidden_size={self.hidden_size},\n",
    "        vocabulary_size={self.vocabulary_size},\n",
    "        context_length={self.context_length},\n",
    "        output_dim={self.output_dim},\n",
    "        out_features={self.output_features},\n",
    "        (token_embedding_layer)={self.token_embedding_layer}),\n",
    "        (pos_embedding_layer)={self.pos_embedding_layer},\n",
    "        (rnn_layer)={self.rnn_layers},\n",
    "        (output_layer)={self.output_layer}\n",
    "        )\n",
    "        '''\n",
    "        return repr\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        token_embeddings_ = self.token_embedding_layer(X)\n",
    "        pos_embeddings_ = self.pos_embedding_layer(torch.arange(self.context_length).to(device=device))\n",
    "        input_embeddings_ = token_embeddings_ + pos_embeddings_\n",
    "\n",
    "        states = None\n",
    "        for rnn_layer in self.rnn_layers:\n",
    "            output_, states = rnn_layer(input_embeddings_, states)\n",
    "        \n",
    "        # print(f\"Output from LSTM shape: {output_.shape}\")\n",
    "        output_ = self.output_layer(output_)\n",
    "        # print(f\"Output RNN shape: {output_.shape}\")\n",
    "\n",
    "        # output_ = torch.softmax(output_, dim=-1)\n",
    "        # output_ = torch.argmax(output_, dim=-1)\n",
    "        \n",
    "        return output_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 153349201\n",
      "Total memory footprint required to run the model: 584.98 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNModel(input_size=1024,\n",
       "        hidden_size=1024,\n",
       "        vocabulary_size=50257,\n",
       "        context_length=16,\n",
       "        output_dim=1024,\n",
       "        out_features=50257,\n",
       "        (token_embedding_layer)=Embedding(50257, 1024)),\n",
       "        (pos_embedding_layer)=Embedding(16, 1024),\n",
       "        (rnn_layer)=ModuleList(\n",
       "  (0): LSTMText(id=0, input_size=1024, hidden_size=1024)\n",
       "  (1): LSTMText(id=1, input_size=1024, hidden_size=1024)\n",
       "  (2): LSTMText(id=2, input_size=1024, hidden_size=1024)\n",
       "  (3): LSTMText(id=3, input_size=1024, hidden_size=1024)\n",
       "  (4): LSTMText(id=4, input_size=1024, hidden_size=1024)\n",
       "  (5): LSTMText(id=5, input_size=1024, hidden_size=1024)\n",
       "),\n",
       "        (output_layer)=Linear(in_features=1024, out_features=50257, bias=True)\n",
       "        )\n",
       "        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(input_size=token_embedding_dim,\n",
    "                 hidden_size=output_dim,\n",
    "                 vocabulary_size=vocabulary_size,\n",
    "                 context_length=context_length,\n",
    "                 output_dim=output_dim,\n",
    "                 out_features=vocabulary_size,\n",
    "                 num_layers=num_layers)\n",
    "model.to(device=device)\n",
    "model.eval()\n",
    "\n",
    "total_params: int = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "total_size_mb: int = (total_params * 4) / (1024 * 1024)\n",
    "print(\"Total memory footprint required to\"\n",
    "          f\" run the model: {total_size_mb:,.2f} MB\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim_sequence(input: torch.Tensor, context_length: int):\n",
    "    input = input[:, :context_length]\n",
    "    output = F.pad(input, (0, context_length - input.shape[1]), \"constant\", 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,\n",
    "                         idx: torch.Tensor,\n",
    "                         max_new_tokens,\n",
    "                         context_size):\n",
    "    \n",
    "    if idx.shape[-1] < context_size:\n",
    "        idx = pad_or_trim_sequence(idx, context_length=context_size)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            output_ = model(idx_cond)\n",
    "        \n",
    "        output_ = torch.softmax(output_, dim=-1)\n",
    "        output_ = torch.argmax(output_, dim=-1)\n",
    "        # print(f\"Output from model shape before cat: {output_.shape}\")\n",
    "        # print(f\"output_[:, -1]: {output_[:, 2].unsqueeze(1).shape}\")\n",
    "        # print(f\"Input generate_text_simple idx: {idx.shape}\")\n",
    "        idx = torch.cat((idx, output_[:, -1].unsqueeze(1)), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(texts, tokenizer):\n",
    "    encoded_tensors = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})    \n",
    "        encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "        encoded_tensors.append(encoded_tensor)\n",
    "    return_tensor = torch.cat(encoded_tensors, dim=0).to(device=device)\n",
    "    return return_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    response = []\n",
    "    for batch in range(token_ids.shape[0]):\n",
    "        response.append(\n",
    "            tokenizer.decode(\n",
    "                token_ids[batch, :].squeeze(0).tolist()\n",
    "                )\n",
    "            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = [\"He was always thinking\"] #He was always thinking what the estate would be if those mortgages could but be paid off.\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# every effort moves you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " ['He was always thinking!!!!!!!!!!!!urrectionPatパsterdamBritish35 DemocratPatパ35 outrage�Catalog!? ninety']\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    context_size=context_length\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.2 Generator to enabling split dataset into train and validation subsets\n",
    "generator1: torch.Generator = torch.Generator().manual_seed(918)\n",
    "\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, context_length, stride)\n",
    "\n",
    "# 1.3 Creates a list with both subsets, 90% training, 10% evaluation\n",
    "datasets: List[Subset] = torch.utils.data.random_split(\n",
    "    dataset,\n",
    "    [0.9, 0.1],\n",
    "    generator=generator1\n",
    ")\n",
    "\n",
    "# 1.4 Assigns train and validation datasets accordingly\n",
    "train_dataset: Subset = datasets[0]\n",
    "validation_dataset: Subset = datasets[1]\n",
    "\n",
    "train_dataloader: DataLoader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "num_batches: int = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "evaluator = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for input_batch, target_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_batch = input_batch.to(device=device)\n",
    "        target_batch = target_batch.to(device=device)\n",
    "        output = model(input_batch)\n",
    "\n",
    "        output_flatten: torch.Tensor = torch.flatten(output, 0, 1)\n",
    "        target_batch_flatten: torch.Tensor = torch.flatten(target_batch, 0, 1)\n",
    "        # print(f\"Model output flatten shape: {output_flatten.shape}\")\n",
    "        # print(f\"target_batch flatten shape: {target_batch_flatten.shape}\")\n",
    "        loss: torch.Tensor = evaluator(output_flatten, target_batch_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter % 10000 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.6f} Batches: {counter}/{num_batches}')\n",
    "        counter += 1\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    model.eval()\n",
    "    token_ids = generate_text_simple(\n",
    "                    model=model,\n",
    "                    idx=text_to_token_ids(start_context, tokenizer),\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    context_size=context_length\n",
    "                )\n",
    "    print(\"Output text: \", token_ids_to_text(token_ids, tokenizer))\n",
    "    model.train()\n",
    "    print(f\"End of epoch: {epoch} Loss: {loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
