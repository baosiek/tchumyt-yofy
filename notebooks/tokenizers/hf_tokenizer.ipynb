{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size = 20000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = load_dataset('Salesforce/wikitext', \"wikitext-103-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1801350\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"{wiki_dataset}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wiki_dataset[\"train\"]\n",
    "test = wiki_dataset[\"test\"]\n",
    "validation = wiki_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator_wiki(train, test, validation):\n",
    "  for data in [train, test, validation]:\n",
    "    for i, data in enumerate(data):\n",
    "      text = data.get(\"text\", None) \n",
    "      if isinstance(text, str): # gotten object maybe not string\n",
    "        text = text.strip() # removes leading and trailing white spaces\n",
    "        if len(text) > 0: # only text with content\n",
    "          if text.startswith(\"=\"): # removes '=' from heads\n",
    "            text = text.replace(\"=\", \"\").strip()\n",
    "          yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"([A-Z][a-z]+)\"\n",
    "\n",
    "with open(\"../../llm/resources/emoji_list.txt\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "    emojis = []\n",
    "    for emoji in content:\n",
    "        emoji: str = emoji.strip().split(\"\\t\")\n",
    "        if len(emoji) > 1:\n",
    "            emoji = emoji[1]\n",
    "        else:\n",
    "            if emoji[0].isdigit():\n",
    "                continue\n",
    "            if re.search(regex, emoji[0]):\n",
    "                continue\n",
    "            emoji = emoji[0]\n",
    "\n",
    "        emojis.append(emoji + \"\\n\")\n",
    "\n",
    "with open(\"../../llm/resources/emoji_list_ok.txt\", \"w\") as f:\n",
    "        f.writelines(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2796\n"
     ]
    }
   ],
   "source": [
    "emojiz = []\n",
    "with open(\"../../llm/resources/emoji_list_ok.txt\", \"r\") as f:\n",
    "        emojiz = f.readlines()\n",
    "print(len(emojiz))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(emojiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train_from_iterator(iterator_wiki(\n",
    "    train, test, validation), trainer=trainer)\n",
    "tokenizer.save(\"/home/baosiek/Projects/tchumyt-yofy/llm/resources/wiki_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer.from_file(\"/home/baosiek/Projects/tchumyt-yofy/llm/resources/wiki_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '😎', '😂', '😀', '[UNK]', '[UNK]', 'all', '?']\n",
      "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 30000, 30001, 30002, 0, 0, 5097, 35]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer1.encode(\"Hello, y'all! How are you 😎 😂 😀 😁 😈 all?\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello , y ' all ! How are you 😎 😂 😀 all ?\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_output = tokenizer1.decode(output.ids)\n",
    "new_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
